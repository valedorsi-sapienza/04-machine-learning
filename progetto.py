# -*- coding: utf-8 -*-
"""Progetto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10VjPg4WEN5Zn_z5gj1O06jTCyZ8WrVEb

# Re-basin ortogonale:una nuova prospettiva sulla connettività lineare tra modelli neurali

Author:
- Valeria d'Orsi, 2136945

# Imports and utilities
"""

#@title Import dependencies

import torch
import numpy as np
import random
import torch.nn.functional as F

from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from tqdm import tqdm
from torch import nn
from torch import optim

torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

"""Al fine di effettuare un confronto rigoroso con quanto riportato nell’articolo *Git Re-Basin*, si considerano architetture di tipo Multi-Layer Perceptron (MLP) composte da tre layer nascosti, ciascuno costituito da 512 unità. Tra i layer viene utilizzata la funzione di attivazione ReLU, mentre non viene applicata alcuna forma di normalizzazione. Questa configurazione consente di analizzare in modo controllato le dinamiche di allineamento tra modelli, in coerenza con il setting sperimentale di riferimento.

"""

layer_names = ['fc1', 'fc2', 'fc3']

import matplotlib.pyplot as plt
def train(model,train_dl,valid_dl,save_prefix):
      loss_func = F.cross_entropy
      epochs = 12
      lr = 0.001
      losses = torch.zeros(epochs)
      val_losses = torch.zeros(epochs)
      opt = optim.Adam(model.parameters(), lr=lr)

      valid_accuracy = torch.zeros(epochs)

      for epoch in range(epochs):
          model.train()
          train_loss = 0
          total = 0
          for xb, yb in train_dl:
              pred = model(xb)
              loss = loss_func(pred, yb)
              loss.backward()
              opt.step()
              opt.zero_grad()
              train_loss += loss.item() * xb.size(0)
              total += xb.size(0)
          losses[epoch] = train_loss / total


          model.eval()
          val_loss = 0
          total = 0
          correct= 0
          with torch.no_grad():
              for xv, yv in valid_dl:
                  outputs = model(xv)
                  val_loss += loss_func(outputs, yv).item() * xv.size(0)
                  predicted = outputs.argmax(dim=1)
                  correct += (predicted == yv).sum().item()
                  total += yv.size(0)
              valid_accuracy[epoch] = correct / total
              val_losses[epoch] = val_loss / total

          if not epoch % 10:
              print(epoch, loss.item())

      plt.figure(figsize=(6, 3))
      plt.plot(valid_accuracy, label='validation', color='red')
      plt.xlabel('epochs')
      plt.ylabel('accuracy')
      plt.xlim(-1, epochs + 1)
      plt.ylim(0.1, 1)
      plt.legend()
      plt.grid(True)
      plt.show()
      plt.figure(figsize=(6, 3))
      plt.plot(losses, color='red', label='train')
      plt.plot(val_losses.detach().numpy(), color='black', label='val')
      plt.xlabel('epochs')
      plt.ylabel('loss')
      plt.grid(True)
      plt.legend()
      plt.show()

      print(f"final validation accuracy: {valid_accuracy[-1]*100:.2f}%")

import copy
def interpolate_models(model_a, model_b, alpha):
    new_model = copy.deepcopy(model_a)
    with torch.no_grad():
        for (pa, pb, pc) in zip(model_a.parameters(), model_b.parameters(), new_model.parameters()):
            pc.data = (1 - alpha) * pa.data + alpha * pb.data
    return new_model

def evaluate_loss(model, dataloader, loss_fn):
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for xv, yv in dataloader:
            outputs = model(xv)
            val_loss += loss_fn(outputs, yv).item() * xv.size(0)

            predicted = outputs.argmax(dim=1)
            correct += (predicted == yv).sum().item()
            total += yv.size(0)
    avg_loss = val_loss / total
    accuracy = correct / total
    return avg_loss, accuracy

def loss_barrier(model_a,model_b,valid_loader,alphas,loss_fn):
        loss_a, _ = evaluate_loss(model_a, valid_loader, loss_fn)
        loss_b, _ = evaluate_loss(model_b, valid_loader, loss_fn)
        base_loss=(loss_a+loss_b)*0.5
        max_interpolated_loss = -float('inf')
        media=0
        for alpha in alphas:
                model_interp = interpolate_models(model_a, model_b, alpha)
                model_interp.eval()
                loss_interpolated, _ = evaluate_loss(model_interp, valid_loader, loss_fn)
                barrier = loss_interpolated - base_loss
                max_interpolated_loss = max(max_interpolated_loss, barrier)
                media+=loss_interpolated
        media=media/len(alphas)
        return round(max_interpolated_loss, 4),media


lambdas = np.linspace(0, 1, 25)
loss_fn = torch.nn.CrossEntropyLoss()

def comparison_naive(model_a,model_b,valid_dl,loss_fn):
    val_losses_ab ,acc_ab= [],[]
    for lam in lambdas:
        interpolated = interpolate_models(model_a, model_b, lam)
        loss,acc = evaluate_loss(interpolated, valid_dl, loss_fn)
        acc_ab.append(acc)
        val_losses_ab.append(loss)
    return  val_losses_ab ,acc_ab


def get_multiple_fc_activations(model, layer_names, dataloader, max_batches=5):
    import gc

    activations_dict = {name: [] for name in layer_names}

    def get_hook(name):
        def hook_fn(module, input, output):
            activations_dict[name].append(output.detach().cpu())
        return hook_fn

    handles = []
    for name, module in model.named_modules():
        if name in layer_names:
            handle = module.register_forward_hook(get_hook(name))
            handles.append(handle)

    model = model.to("cpu")
    model.eval()

    with torch.no_grad():
        for i, (inputs, _) in enumerate(dataloader):
            if i >= max_batches:
                break
            inputs = inputs.to("cpu")
            model(inputs)


    for handle in handles:
        handle.remove()


    gc.collect()
    torch.cuda.empty_cache()

    Z_dict = {}
    for name, acts in activations_dict.items():
        all_acts = torch.cat(acts, dim=0)
        Z = all_acts.T
        Z_dict[name] = Z

    return Z_dict

def comparison_interpolation_method(model_a,model_b,perm_model,lambdas,valid_dl,loss_fn,stringa,numero):
    val_losses_ab ,acc_ab= [],[]
    val_losses_ap ,acc_ap= [],[]
    for lam in lambdas:
        interpolated = interpolate_models(model_a, model_b, lam)
        loss,acc = evaluate_loss(interpolated, valid_dl, loss_fn)
        acc_ab.append(acc)
        val_losses_ab.append(loss)

        interpolated_p = interpolate_models(model_a, perm_model, lam)
        loss_p,acc_p = evaluate_loss(interpolated_p, valid_dl, loss_fn)
        acc_ap.append(acc_p)
        val_losses_ap.append(loss_p)
    if numero=='1':
        fig, axs = plt.subplots(1, 2, figsize=(14, 6))
    
        axs[0].plot(lambdas, val_losses_ab, marker='o', label="Naive A ↔ B")
        axs[0].plot(lambdas, val_losses_ap, marker='s', label=stringa)
        axs[0].set_xlabel("λ (interpolation factor)")
        axs[0].set_ylabel("Validation Loss")
        axs[0].set_title("Validation Loss - Interpolation Between Models")
        axs[0].grid(True)
        axs[0].legend()
    
        axs[1].plot(lambdas, acc_ab, marker='o', label=" Naive A ↔ B")
        axs[1].plot(lambdas, acc_ap, marker='s', label=stringa)
        axs[1].set_xlabel("λ (interpolation factor)")
        axs[1].set_ylabel("Accuracy")
        axs[1].set_title("Accuracy - Interpolation Between Models")
        axs[1].grid(True)
        axs[1].legend()
    
        plt.tight_layout()
        plt.show()

    return val_losses_ap ,acc_ap

#@title Per un futuro confronto
Losses,media_losses,method=[],[],[]
def aggiorna(loss,media,stringa):
  Losses.append(loss)
  media_losses.append(media)
  method.append(stringa)

"""# PERMUTATION SELECTION METHODS

## Matching activations
"""

from scipy.optimize import linear_sum_assignment

def compute_permutation_matrix(Z_A: torch.Tensor, Z_B: torch.Tensor) :
    Z_A_np = Z_A.cpu().numpy() #512,640
    Z_B_np = Z_B.cpu().numpy()

    d = Z_A_np.shape[0]
    cost_matrix = np.linalg.norm(Z_A_np[:, None, :] - Z_B_np[None, :, :], axis=2)**2

    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    P = np.zeros((d, d), dtype=np.float32)
    P[row_ind, col_ind] = 1.0

    return torch.from_numpy(P)

def residual_misalignment_error(Z_A, Z_B, P):
    Z_diff = Z_A - P @ Z_B
    return (Z_diff**2).sum().item() / Z_A.numel()

def permute_activations(model_a, model_b, layer_names, valid_dl):
    perm_model = copy.deepcopy(model_b)
    diz_res,diz_perm={},{}
    dizA = get_multiple_fc_activations(model_a, layer_names, valid_dl)
    dizB = get_multiple_fc_activations(model_b, layer_names, valid_dl)
    P_prev=None
    for n,layer in enumerate(layer_names):
        Z_A = dizA[layer]
        Z_B = dizB[layer]
        Player = compute_permutation_matrix(Z_A, Z_B)
        diz_res[layer]=residual_misalignment_error(Z_A, Z_B, Player)

        diz_perm[layer]=Player

        layer_obj = getattr(model_b, layer)
        W = layer_obj.weight.data.clone()
        b = layer_obj.bias.data.clone()

        if n>0:
          W_new = Player @ W @ P_prev.T
        else:
          W_new = Player @ W

        b_new = Player @ b
        P_prev = Player

        layer_perm = getattr(perm_model, layer)
        with torch.no_grad():
            layer_perm.weight.data.copy_(W_new)
            layer_perm.bias.data.copy_(b_new)

    final_layer_name = 'fc4'
    final_layer_model_b = getattr(model_b, final_layer_name)
    W_final = final_layer_model_b.weight.data.clone()
    b_final = final_layer_model_b.bias.data.clone()

    final_layer_perm = getattr(perm_model, final_layer_name)
    with torch.no_grad():
        final_layer_perm.weight.data.copy_(W_final @ P_prev.T)
        final_layer_perm.bias.data.copy_(b_final)

    return perm_model,diz_perm


"""## Matching weights

"""

from typing import NamedTuple
from collections import defaultdict

class PermutationSpec(NamedTuple):
    perm_to_axes: dict   # mappa permutazioni a (layer, asse)
    axes_to_perm: dict   # mappa layer a permutazioni


def permutation_spec_from_axes_to_perm(axes_to_perm: dict) -> PermutationSpec:
    perm_to_axes = defaultdict(list)
    for layer, axis_perms in axes_to_perm.items():        #axis_perm è una tupla con le permutazioni da attuare per il layer:(perm_riga,perm_colonne)
        for axis, perm in enumerate(axis_perms):
            if perm is not None:
                perm_to_axes[perm].append((layer, axis))
    return PermutationSpec(perm_to_axes=dict(perm_to_axes), axes_to_perm=axes_to_perm)


def mlp_permutation_spec_torch(num_hidden_layers: int) -> PermutationSpec:
    assert num_hidden_layers >= 1

    spec = {}
    spec["fc1.weight"] = ("P_1", None)
    spec["fc1.bias"] = ("P_1",None)
    for i in range(2, num_hidden_layers + 1):
        spec[f"fc{i}.weight"] = (f"P_{i}", f"P_{i-1}")
        spec[f"fc{i}.bias"] = (f"P_{i}",None)
    spec[f"fc{num_hidden_layers + 1}.weight"] = (None, f"P_{num_hidden_layers}")
    spec[f"fc{num_hidden_layers + 1}.bias"] = (None,None)

    return permutation_spec_from_axes_to_perm(spec)


def get_permuted_param(ps: PermutationSpec, perm, k: str, params, except_axis=None):
    w = params[k]
    for axis, p in enumerate(ps.axes_to_perm[k]):
        if axis == except_axis:
            continue
        if p is not None:
            if axis == 0:
                # permuta le righe (neuroni in uscita)
                w = w[perm[p], ...]
            else:
                # permuta le colonne (neuroni in ingresso)
                w = w[:, perm[p], ...]
    return w


def apply_permutation(ps: PermutationSpec, perm, params):
    """Applica le permutazioni `perm` ai parametri `params`."""
    return {k: get_permuted_param(ps, perm, k, params) for k in params.keys()}


def weight_matching(rng, ps: PermutationSpec, params_a: dict,params_b: dict, max_iter=100, init_perm=None, silent=False):

    perm_sizes = {p: params_a[axes[0][0]].shape[axes[0][1]] for p, axes in ps.perm_to_axes.items()}
    if init_perm is None:
        perm = {p: torch.arange(n) for p, n in perm_sizes.items()}
    else:
        perm = init_perm

    perm_names = list(perm.keys())

    for iteration in range(max_iter):
        progress = False

        indices = torch.randperm(len(perm_names), generator=rng)
        perm_names = [perm_names[i] for i in indices]

        for p in perm_names:
            n = perm_sizes[p]
            A = torch.zeros((n, n), dtype=torch.float32)

            for wk, axis in ps.perm_to_axes[p]:
                w_a = params_a[wk]
                w_b = get_permuted_param(ps, perm, wk, params_b, except_axis=axis)

                w_a = w_a.transpose(0, axis).reshape(n, -1)
                w_b = w_b.transpose(0, axis).reshape(n, -1)

                A += w_a @ w_b.T

            # Risolvi problema di assegnamento lineare per massimizzare similarità
            A_np = A.detach().numpy()
            ri, ci = linear_sum_assignment(A_np, maximize=True)

            assert np.all(ri == np.arange(n)), "Matching righe non ordinate come atteso"

            oldL = A[torch.arange(n), perm[p]].sum()
            newL = A[torch.arange(n), torch.tensor(ci)].sum()

            #if not silent:
                #print(f"Iter {iteration} | {p}: ΔL = {(newL - oldL).item():.6f}")

            if newL > oldL + 1e-12:
                perm[p] = torch.tensor(ci)
                progress = True

        if not progress:
            break

    return perm

#perm = {"P_0": tensor([...]), "P_1": tensor([...]),..}

def permute_model_1(model_b,params_b,state_b, ps, perm):
    permuted_params_b = apply_permutation(ps, perm, params_b)
    permuted_state_b = state_b.copy()
    for k in permuted_params_b:
        permuted_state_b[k] = permuted_params_b[k]
    m_perm = copy.deepcopy(model_b)
    m_perm.load_state_dict(permuted_state_b)
    return m_perm


def permutation_matrix(perm):
    N = perm.size(0)
    P = torch.zeros(N, N)
    for i in range(N):
        P[i, perm[i]] = 1
    return P

def create_dict( perm,layer_names=['fc1','fc2','fc3']):
    permutazioni=[permutation_matrix(p) for p in perm.values()]
    diz_perm={}
    for i,layer in enumerate(layer_names):
      diz_perm[layer]=permutazioni[i]
    return diz_perm


"""###  Matching weights: 2ⁿᵈ algorithm

"""

def solve_lap(cost_matrix):
    """Solve the linear assignment problem, return permutation indices"""
    row_ind, col_ind = linear_sum_assignment(-cost_matrix)
    return col_ind

def permutation_matrix(perm):
    n = len(perm)
    P = torch.zeros(n, n)
    P[torch.arange(n), perm] = 1.0
    return P

def permutation_coordinate_descent(weights_A, weights_B, max_iter=10):
    L = len(weights_A)
    d = weights_A[1].shape[0]

    perms = [torch.eye(d) for _ in range(L - 1)]  # P1, P2, ..., P_{L-2}

    for _ in range(max_iter):
        for l in torch.randperm(L - 1):  # layers 0, 1, ..., L-2 (exclude output)

            # P_{l-1}, P_{l+1}
            P_l0 = perms[l - 1] if l > 0 else torch.eye(weights_A[0].shape[1])  # P0 = identity(784)
            P_l1 = perms[l + 1] if l < L - 2 else torch.eye(weights_A[-1].shape[0])  # P_{L-1} = identity(10)
            WA_l = weights_A[l]
            WA_l1 = weights_A[l + 1]
            WB_l = weights_B[l]
            WB_l1 = weights_B[l + 1]

            M = WA_l @ P_l0 @ WB_l.T + WA_l1.T @ P_l1 @ WB_l1
            M_np = M.detach().cpu().numpy()
            perm = solve_lap(M_np)
            perms[l] = permutation_matrix(torch.tensor(perm))

    return perms


def get_weights(model):
    weights = [model.fc1.weight.data.clone(),model.fc2.weight.data.clone(),model.fc3.weight.data.clone(),model.fc4.weight.data.clone()]
    return weights

def get_biases(model):
    biases = [model.fc1.bias.data.clone(),model.fc2.bias.data.clone(),model.fc3.bias.data.clone(),model.fc4.bias.data.clone()]
    return biases


def apply_perm2(model_b, perm, layer_names):
    weighted_model = copy.deepcopy(model_b)

    weights_B = get_weights(model_b)
    biases_B = get_biases(model_b)
    perm = [torch.eye(weights_B[0].shape[1])] + perm + [torch.eye(weights_B[-1].shape[0])]

    for i, layer_name in enumerate(layer_names):
        W_new = perm[i + 1] @ weights_B[i] @ perm[i].T
        b_new = perm[i + 1] @ biases_B[i]
        layer = getattr(weighted_model, layer_name)
        with torch.no_grad():
            layer.weight.copy_(W_new)
            layer.bias.copy_(b_new)

    return weighted_model



"""## Learning permutations with a straight-through estimator"""

def average_models(model_A, model_B):
    avg_model = copy.deepcopy(model_A)
    with torch.no_grad():
        for param_A, param_B, param_avg in zip(model_A.parameters(), model_B.parameters(), avg_model.parameters()):
            param_avg.data.copy_(0.5 * (param_A.data + param_B.data))
    return avg_model


def permute_model2(model_b,params_b,state_b, ps, perm):
    permuted_params_b = apply_permutation(ps, perm, params_b)
    permuted_state_b = state_b.copy()
    for k in permuted_params_b:
        permuted_state_b[k] = permuted_params_b[k]
    m_perm = copy.deepcopy(model_b)
    m_perm.load_state_dict(permuted_state_b)
    return m_perm

def straight_through_training(model_a, model_b, data_loader, loss_fn, ps, rng, lr=1e-3, max_iter=50):
    tilde_b = copy.deepcopy(model_a)
    optimizer = torch.optim.SGD(tilde_b.parameters(), lr=lr)

    for step in range(max_iter):
        state_tilde_b = tilde_b.state_dict()
        state_b = model_b.state_dict()

        params_tilde_b = {k: v for k, v in state_tilde_b.items() if 'weight' in k or 'bias' in k}
        params_b = {k: v for k, v in state_b.items() if 'weight' in k or 'bias' in k}
        perm = weight_matching(torch.Generator().manual_seed(111), ps, params_tilde_b, params_b, silent=False)
        permuted_model_b = permute_model2(model_b, params_b,state_b, ps, perm)

        permuted_model_b.eval()

        for inputs, targets in data_loader:
            optimizer.zero_grad()
            with torch.no_grad():
                outputs_projB = permuted_model_b(inputs)

            outputs_tildeB = tilde_b(inputs)
            outputs_ste = outputs_projB.detach() + (outputs_tildeB - outputs_projB.detach())

            loss = loss_fn(outputs_ste, targets)

            loss.backward()
            optimizer.step()



        print(f"Step {step + 1}/{max_iter} | Loss midpoint: {loss:.6f}")

    return tilde_b, perm



"""# Slerp"""

def slerp_models_layerwise(model1, model2, t):
    import copy
    model_interp = copy.deepcopy(model1)
    with torch.no_grad():
        for p1, p2, p_interp in zip(model1.parameters(), model2.parameters(), model_interp.parameters()):
            v1 = p1.data.flatten()
            v2 = p2.data.flatten()

            v1_norm = v1 / v1.norm()
            v2_norm = v2 / v2.norm()
            dot = torch.clamp(torch.dot(v1_norm, v2_norm), -1.0, 1.0)
            theta = torch.acos(dot)
            sin_theta = torch.sin(theta)

            if sin_theta < 1e-6:
                v_interp = (1.0 - t) * v1 + t * v2
            else:
                v_interp_norm = (torch.sin((1 - t) * theta) / sin_theta) * v1_norm + \
                               (torch.sin(t * theta) / sin_theta) * v2_norm
                norm_interp = (1 - t) * v1.norm() + t * v2.norm()
                v_interp = v_interp_norm * norm_interp

            p_interp.data.copy_(v_interp.view_as(p_interp))

    return model_interp



#layerwise è peggio

def slerp_models(model1, model2, t):
    """ SLERP interpolation between two PyTorch models """
    v1 = torch.cat([p.data.flatten() for p in model1.parameters()])
    v2 = torch.cat([p.data.flatten() for p in model2.parameters()])
    v1_copy = v1.clone()
    v2_copy = v2.clone()

    v1_norm = v1_copy / v1_copy.norm()
    v2_norm = v2_copy / v2_copy.norm()

    dot = torch.clamp(torch.dot(v1_norm, v2_norm), -1.0, 1.0)
    theta = torch.acos(dot)
    sin_theta = torch.sin(theta)
    if sin_theta < 1e-6:
        v_interp = (1.0 - t) * v1 + t * v2
    else:
        v_interp_norm = (torch.sin((1 - t) * theta) / sin_theta) * v1_norm + \
                       (torch.sin(t * theta) / sin_theta) * v2_norm
        norm_interp = (1 - t) * v1.norm() + t * v2.norm()
        v_interp = v_interp_norm * norm_interp

    model_interp = copy.deepcopy(model1)
    offset = 0
    for p in model_interp.parameters():
        numel = p.numel()
        p.data.copy_(v_interp[offset:offset + numel].view_as(p))
        offset += numel

    return model_interp


def interpolate_models_slerp(model_a, model_b, reflected_model, alphas,valid_loader):
    val_losses_ab,acc_ab=[],[]
    val_losses_ar,acc_ar=[],[]
    for lam in lambdas:
        interpolated = interpolate_models(model_a, model_b, lam)
        loss,acc = evaluate_loss(interpolated, valid_loader, loss_fn)
        val_losses_ab.append(loss)

        interpolated_r = slerp_models(model_a, reflected_model, lam)
        loss_r,acc_r = evaluate_loss(interpolated_r, valid_loader, loss_fn)
        val_losses_ar.append(loss_r)
        acc_ar.append(acc_r)

    plt.plot(lambdas, val_losses_ab, marker='o', label="A ↔ B")
    plt.plot(lambdas, val_losses_ar, marker='s', label="A ↔ Slerp(B)")
    plt.plot(lambdas, acc_ar, marker='s', label="A ↔ Slerp(B)")
    plt.xlabel("λ (interpolation factor)")
    plt.ylabel("Validation Loss")
    plt.title("Validation Loss - Interpolation Between Models")
    plt.grid(True)
    plt.legend()
    plt.show()

def loss_barrier_slerp(model_a,model_b,valid_loader,alphas,loss_fn):
        loss_a, _ = evaluate_loss(model_a, valid_loader, loss_fn)
        loss_b, _ = evaluate_loss(model_b, valid_loader, loss_fn)
        base_loss=(loss_a+loss_b)*0.5
        max_interpolated_loss = -float('inf')
        media=0
        for alpha in alphas:
                model_interp = interpolate_models_slerp(model_a, model_b, alpha)
                model_interp.eval()
                loss_interpolated, _ = evaluate_loss(model_interp, valid_loader, loss_fn)
                barrier = loss_interpolated - base_loss
                max_interpolated_loss = max(max_interpolated_loss, barrier)
                media+=loss_interpolated
        media=media/len(alphas)
        return round(max_interpolated_loss, 4),media

"""# SVD PESI"""

def find_rotation_left(W_a, W_b):
    M = W_a @ W_b.T
    U, _, Vh = torch.linalg.svd(M)
    R = U @ Vh
    return R


def svd_pesi(model_a, model_b, layer_names):
    rotated_weight_b = copy.deepcopy(model_b)
    R_prev = None
    diz_rot={}
    for n, layer in enumerate(layer_names):
        W_a = getattr(model_a, layer).weight.data
        W_b = getattr(model_b, layer).weight.data
        b_b = getattr(model_b, layer).bias.data

        if n == 0:
            R= find_rotation_left(W_a, W_b)
            W_new = R @ W_b
            b_new = R @ b_b
        else:
            R= find_rotation_left(W_a, W_b @R_prev.T)
            W_new = R @ W_b @ R_prev.T
            b_new = R @ b_b
        diz_rot[layer]=R
        R_prev = R
        layer_obj = getattr(rotated_weight_b, layer)
        with torch.no_grad():
            layer_obj.weight.copy_(W_new)
            layer_obj.bias.copy_(b_new)

    final_layer_name = 'fc4'
    W_final = getattr(model_b, final_layer_name).weight.data.clone()
    b_final = getattr(model_b, final_layer_name).bias.data.clone()

    with torch.no_grad():

        getattr(rotated_weight_b, final_layer_name).weight.copy_( W_final @ R_prev.T)
        getattr(rotated_weight_b, final_layer_name).bias.copy_(b_final)

    return rotated_weight_b,diz_rot



"""# Stiefel"""

def project_to_stiefel(R):
    with torch.no_grad():
        U, _, Vh = torch.linalg.svd(R, full_matrices=False)
        return U @ Vh

def alignment_loss(R, Z_A, Z_B):
    return torch.norm(Z_A - R@ Z_B , p='fro') ** 2 / Z_A.size(1)


def optimize_rotation(Z_A, Z_B, n_epochs=30, lr=0.01):
    d = Z_A.shape[0]
    R = nn.Parameter(torch.eye(d))
    optimizer = torch.optim.SGD([R], lr=lr)
    for _ in range(n_epochs):
        loss = alignment_loss(R, Z_A, Z_B)
        loss.backward()
        optimizer.step()
        R.data = project_to_stiefel(R)
        R.grad.zero_()
    return R.detach()


def stiefel(model_a, model_b, layer_names,valid_dl):
    rotated_weight_b = copy.deepcopy(model_b)
    dizA = get_multiple_fc_activations(model_a, layer_names, valid_dl)
    dizB = get_multiple_fc_activations(model_b, layer_names, valid_dl)
    diz={}
    R_prev=None
    for n,layer in enumerate(layer_names):
        Z_A = dizA[layer]
        Z_B = dizB[layer]
        W_a = getattr(model_a, layer).weight.data
        W_b = getattr(model_b, layer).weight.data
        b_b = getattr(model_b, layer).bias.data


        if n == 0:
            R= optimize_rotation(W_a,W_b)
            W_new = R @ W_b
        else:
            R= optimize_rotation(W_a,W_b @ R_prev.T)
            W_new = R @ W_b @ R_prev.T
        b_new = R @ b_b
        R_prev = R
        diz[layer]=R
        layer_obj = getattr(rotated_weight_b, layer)
        with torch.no_grad():
            layer_obj.weight.copy_(W_new)
            layer_obj.bias.copy_(b_new)
    final_layer_name = 'fc4'
    W_final = getattr(model_b, final_layer_name).weight.data.clone()
    b_final = getattr(model_b, final_layer_name).bias.data.clone()

    with torch.no_grad():
        getattr(rotated_weight_b, final_layer_name).weight.copy_( W_final @ R_prev.T)
        getattr(rotated_weight_b, final_layer_name).bias.copy_(b_final)

    return rotated_weight_b,diz


"""# Stiefel studiato"""

def skew_symmetric_W(G, X):
    return G @ X.T - X @ G.T

def cayley_transform(X, G, tau):
    W = skew_symmetric_W(G, X)
    n = W.shape[0]
    I = torch.eye(n, device=X.device, dtype=X.dtype)
    M = I + (tau / 2) * W
    M_inv = torch.linalg.inv(M)
    C = M_inv @ (I - (tau / 2) * W)
    return C @ X, M_inv

def f(X, A, B):
    return torch.norm(A - X @ B, p='fro')**2

def grad_f(X, A, B):
    return -2 * (A - X @ B) @ B.T

def cayley_search(F, X, G, tau_init=1.0, rho1=1e-4, rho2=0.9, max_iter=20):
    n = X.shape[0]
    I = torch.eye(n)
    tau = tau_init
    FX0 = F(X)
    W = skew_symmetric_W(G, X)
    F0_prime = torch.trace(G.T @ (-W @ X))

    for _ in range(max_iter):
        Y_tau, M_inv = cayley_transform(X, G, tau)
        Y_prime = -M_inv @ W @ ((X + Y_tau) / 2)

        F_Y = F(Y_tau)
        F_tau_prime = torch.trace(G.T @ Y_prime)

        if (F_Y <= FX0 + rho1 * tau * F0_prime) and (F_tau_prime >= rho2 * F0_prime):
            return Y_tau, tau

        tau /= 2.0

    return Y_tau, tau

def optimize_on_stiefel(A, B, p, max_iter=50, tol=1e-6, verbose=True):
    n = A.shape[0]
    X = A @ B.T
    X, _ = torch.linalg.qr(X)
    for k in range(max_iter):
        G = grad_f(X, A, B)
        grad_norm = torch.norm(G)
        if grad_norm < tol:
            if verbose:
                print(f"Convergenza raggiunta a iterazione {k}, ||G|| = {grad_norm:.2e}")
            break

        F_func = lambda X_: f(X_, A, B)
        X_new, tau = cayley_search(F_func, X, G)

        if verbose and k % 10 == 0:
            loss = F_func(X)
            print(f"[Iter {k:3d}] Loss: {loss:.4e}, ||G||: {grad_norm:.2e}, tau: {tau:.2e}")

        X = X_new

    return X

def stiefel_model(model_a,model_b,layer_names):
      stiefel_model=copy.deepcopy(model_b)
      diz={}
      R_prev=None
      for n, layer in enumerate(layer_names):
          W_a = getattr(model_a, layer).weight.data
          W_b = getattr(model_b, layer).weight.data
          b_b = getattr(model_b, layer).bias.data
          if n==0:
            R=optimize_on_stiefel(W_a, W_b, 512)
            W_new = R @ W_b
            b_new = R @ b_b
          else:
            R=optimize_on_stiefel(W_a, W_b @R_prev , 512)
            W_new = R @ W_b @ R_prev.T
            b_new =  R @ b_b
          R_prev = R
          assert torch.allclose(R @ R.T, torch.eye(R.shape[0], device=R.device), atol=1e-5), "R non è ortogonale"
          diz[layer]=R
          layer_obj = getattr(stiefel_model, layer)
          with torch.no_grad():
              layer_obj.weight.copy_(W_new)
              layer_obj.bias.copy_(b_new)
      final_layer_name = 'fc4'
      W_final = getattr(model_b, final_layer_name).weight.data.clone()
      b_final = getattr(model_b, final_layer_name).bias.data.clone()

      with torch.no_grad():

          getattr(stiefel_model, final_layer_name).weight.copy_( W_final @ R_prev.T)
          getattr(stiefel_model, final_layer_name).bias.copy_(b_final)

      return stiefel_model,diz



class CayleyOptimizer(nn.Module):
    def __init__(self, A, B, max_iter=50, tol=1e-6, verbose=True):
        super().__init__()
        self.A = A
        self.B = B
        self.max_iter = max_iter
        self.tol = tol
        self.verbose = verbose


        X = A @ B.T
        X, _ = torch.linalg.qr(X)
        self.register_buffer('X', X)

    def skew_symmetric_W(self, G, X):
        return G @ X.T - X @ G.T

    def cayley_transform(self, X, G, tau):
        W = self.skew_symmetric_W(G, X)
        n = W.shape[0]
        I = torch.eye(n, device=X.device, dtype=X.dtype)
        M = I + (tau / 2) * W
        C = torch.linalg.solve(M, I - (tau / 2) * W)
        return C @ X, torch.linalg.inv(M)

    def f(self, X):
        return torch.norm(self.A - X @ self.B, p='fro')**2

    def grad_f(self, X):
        return -2 * (self.A - X @ self.B) @ self.B.T

    def cayley_search(self, X, G, tau_init=10.0, rho1=1e-4, rho2=0.9, max_iter=20):
        n = X.shape[0]
        I = torch.eye(n)
        tau = tau_init
        FX0 = self.f(X)
        W = self.skew_symmetric_W(G, X)
        F0_prime = torch.trace(G.T @ (-W @ X))

        for _ in range(max_iter):
            Y_tau, M_inv = self.cayley_transform(X, G, tau)
            Y_prime = -M_inv @ W @ ((X + Y_tau) / 2)

            F_Y = self.f(Y_tau)
            F_tau_prime = torch.trace(G.T @ Y_prime)

            if (F_Y <= FX0 + rho1 * tau * F0_prime) and (F_tau_prime >= rho2 * F0_prime):
                return Y_tau, tau

            tau /= 2.0

        return Y_tau, tau

    def forward(self):
        X = self.X
        for k in range(self.max_iter):
            G = self.grad_f(X)
            grad_norm = torch.norm(G)
            if grad_norm < self.tol:
                if self.verbose:
                    print(f"Convergenza raggiunta a iterazione {k}, ||G|| = {grad_norm:.2e}")
                break

            X_new, tau = self.cayley_search(X, G)

            if self.verbose and k % 10 == 0:
                loss = self.f(X)
                print(f"[Iter {k:3d}] Loss: {loss:.4e}, ||G||: {grad_norm:.2e}, tau: {tau:.2e}")

            X = X_new

        self.X = X
        return X

def stiefel_model(model_a, model_b, layer_names):
    stiefel_model = copy.deepcopy(model_b)
    R_prev = None

    for n, layer in enumerate(layer_names):
        W_a = getattr(model_a, layer).weight.data
        W_b = getattr(model_b, layer).weight.data
        b_b = getattr(model_b, layer).bias.data
        if n == 0:
            cayley_opt = CayleyOptimizer(W_a, W_b, max_iter=50)
            R = cayley_opt()
            W_new = R @ W_b
            b_new = R @ b_b
        else:
            cayley_opt = CayleyOptimizer(W_a, W_b @ R_prev, max_iter=50)
            R = cayley_opt()
            W_new = R @ W_b @ R_prev.T
            b_new = R @ b_b

        R_prev = R
        assert torch.allclose(R @ R.T, torch.eye(R.shape[0], device=R.device), atol=1e-5), "R non è ortogonale"

        layer_obj = getattr(stiefel_model, layer)
        with torch.no_grad():
            layer_obj.weight.copy_(W_new)
            layer_obj.bias.copy_(b_new)

    final_layer_name = 'fc4'
    W_final = getattr(model_b, final_layer_name).weight.data.clone()
    b_final = getattr(model_b, final_layer_name).bias.data.clone()

    with torch.no_grad():
        getattr(stiefel_model, final_layer_name).weight.copy_(W_final @ R_prev.T)
        getattr(stiefel_model, final_layer_name).bias.copy_(b_final)

    return stiefel_model



"""## Appendice"""

#APPENDICE
def stiefel_attivazioni(model_a, model_b, layer_names,valid_dl):
    rotated_weight_b = MLPModel_mnist()
    dizA = get_multiple_fc_activations(model_a, layer_names, valid_dl)
    dizB = get_multiple_fc_activations(model_b, layer_names, valid_dl)

    for n,layer in enumerate(layer_names):
        Z_A = dizA[layer]
        Z_B = dizB[layer]
        W_a = getattr(model_a, layer).weight.data
        W_b = getattr(model_b, layer).weight.data
        b_b = getattr(model_b, layer).bias.data

        cayley_opt = CayleyOptimizer(Z_A, Z_B, max_iter=50)
        R = cayley_opt()
        if n == 0:
            W_new = R @ W_b
        else:
            W_new = R @ W_b @ R_prev.T
        b_new = b_b
        R_prev = R
        layer_obj = getattr(rotated_weight_b, layer)
        with torch.no_grad():
            layer_obj.weight.copy_(W_new)
            layer_obj.bias.copy_(b_new)

    final_layer_name = 'fc4'
    W_final = getattr(model_b, final_layer_name).weight.data.clone()
    b_final = getattr(model_b, final_layer_name).bias.data.clone()

    with torch.no_grad():
        getattr(rotated_weight_b, final_layer_name).weight.copy_( W_final @ R_prev.T)
        getattr(rotated_weight_b, final_layer_name).bias.copy_(b_final)

    return rotated_weight_b

"""# Procrustes

## Procrustes su attivazioni
"""

#quale è piu affidabile?

def interpolate_models(model_a, model_b, alpha):
    new_model = copy.deepcopy(model_a)
    with torch.no_grad():
        for (pa, pb, pc) in zip(model_a.parameters(), model_b.parameters(), new_model.parameters()):
            pc.data = (1 - alpha) * pa.data + alpha * pb.data
    return new_model

from scipy.linalg import logm, expm

def matrix_log_exp(A_torch, alpha):
    import numpy as np
    from scipy.linalg import logm, expm

    A_np = A_torch.cpu().numpy()
    logA = logm(A_np)
    expA = expm(alpha * logA)

    # Scarta la parte immaginaria (se trascurabile)
    expA_real = expA.real.astype(A_np.dtype)

    return torch.from_numpy(expA_real).to(A_torch.device)

def interpolate_weights_rotational(Wa, Wc, alpha):
    assert Wa.shape == Wc.shape, "Shape mismatch"
    M = Wa @ Wc.T
    U, _, Vt = torch.linalg.svd(M)
    Q = U @ Vt

    # Usa scipy per log e exp
    Q_alpha = matrix_log_exp(Q, alpha)
    Wt = Q_alpha @ Wc
    return Wt


def interpolate_models_rotational(model_a, model_c, alpha):
    model_interp = copy.deepcopy(model_a)
    with torch.no_grad():
        for layer_a, layer_c, layer_i in zip(model_a.modules(), model_c.modules(), model_interp.modules()):
            if isinstance(layer_a, nn.Linear):
                Wa = layer_a.weight.data
                Wc = layer_c.weight.data

                Wt = interpolate_weights_rotational(Wa, Wc, alpha)
                layer_i.weight.data.copy_(Wt)

                # Interpola bias linearmente
                ba = layer_a.bias.data
                bc = layer_c.bias.data
                layer_i.bias.data.copy_((1 - alpha) * ba + alpha * bc)

    return model_interp

def error_non_commutativity(Z_A,Z_B,R):
    E_pre = torch.norm(Z_A - R @ Z_B)**2 / Z_A.numel()
    H_A = torch.relu(Z_A)
    H_B = torch.relu(Z_B)
    E_post = torch.norm(H_A-R @ H_B)**2 / H_A.numel()
    return E_pre,E_post


from scipy.linalg import orthogonal_procrustes

def find_ortogonal(Z_A, Z_B):
    M = Z_A @ Z_B.T  # notare l'ordine: voglio che R @ Z_B ≈ Z_A   512,640*640,512
    U, _, Vt = torch.linalg.svd(M)
    R = U @ Vt
    return R

def find_rotation_SO(Z_A, Z_B):
    M = Z_A @ Z_B.T
    U, _, Vt = torch.linalg.svd(M)
    det = torch.det(U @ Vt)

    D = torch.eye(U.size(0), device=U.device)
    D[-1, -1] = det  # +1 or -1

    R = U @ D @ Vt
    return R


def orthogonal_procrustes_rotate(Z_A, Z_B):
    Z_A_T = Z_A.T.numpy()
    Z_B_T = Z_B.T.numpy()
    R, _ = orthogonal_procrustes(Z_B_T, Z_A_T)
    R=torch.from_numpy(R.T).float()
    return R


def rotate_activations_procrustes(model_a, model_b, layer_names, dataloader,numero,errore):
    rotated_model = copy.deepcopy(model_b)
    R_dict={}
    R_prev=None
    err_pre,err_post,err_relu=[],[],[]
    dizA =get_multiple_fc_activations(model_a, layer_names, dataloader, max_batches=5)  # shape: (n, d)
    dizB = get_multiple_fc_activations(model_b, layer_names,dataloader, max_batches=5)
    for n,layer in enumerate(layer_names):
        Z_A = dizA[layer]  #512,640
        Z_B = dizB[layer]
        if numero==1:
          R=find_ortogonal(Z_A,Z_B)
        elif numero==0:
          R=find_rotation_SO(Z_A, Z_B)
        else:
          R=orthogonal_procrustes_rotate(Z_A, Z_B)
        layer_obj = getattr(model_b, layer)
        W = layer_obj.weight.data.clone()
        b = layer_obj.bias.data.clone()
        if n>0:
          W_new =  R @ W @ R_prev.T
        else:
          W_new = R @ W

        b_new = R@ b  #R@b
        R_dict[layer]=R
        R_prev = R

        layer_rotated = getattr(rotated_model, layer)
        with torch.no_grad():
            layer_rotated.weight.data.copy_(W_new)
            layer_rotated.bias.data.copy_(b_new)

        pre,post=error_non_commutativity(Z_A,Z_B,R)
        err_pre.append(pre.item())
        err_post.append(post.item())
        relu=post-pre
        err_relu.append(relu.item())
    final_layer_name = 'fc4'
    final_layer_model_b = getattr(model_b, final_layer_name)
    W_final = final_layer_model_b.weight.data.clone()
    b_final = final_layer_model_b.bias.data.clone()

    final_layer_rotated = getattr(rotated_model, final_layer_name)
    with torch.no_grad():
        final_layer_rotated.weight.data.copy_(W_final @ R_prev.T)
        final_layer_rotated.bias.data.copy_(b_final)
    if errore==1:
      plt.plot([i for i in range(1,len(layer_names)+1)], err_pre, marker='o', label='Errore pre-attivazioni')
      plt.plot([i for i in range(1,len(layer_names)+1)], err_post, marker='s', label='Errore post-attivazioni')
      plt.plot([i for i in range(1,len(layer_names)+1)], err_relu, marker='^', label='Errore residuo (non linearità)')
      plt.xlabel("Layer")
      plt.ylabel("Errore")
      plt.title("Errori")
      plt.grid(True)
      plt.legend()
      plt.show()

    return rotated_model,R_dict

"""## Procrustes pesi"""

#prova qualcosa di iterativo
from typing import NamedTuple
from collections import defaultdict
from scipy.optimize import linear_sum_assignment

class RotationSpec(NamedTuple):
    rot_to_axes: dict
    axes_to_rot: dict


def rotation_spec_from_axes_to_rot(axes_to_rot: dict) -> RotationSpec:
    rot_to_axes = defaultdict(list)
    for layer, rotations in axes_to_rot.items():        #rotationsè una tupla con le rotazioni da attuare per il layer:(rot sx=0, rot dx=1)
        for lato, rotation in enumerate(rotations):
            if rotation is not None:
                rot_to_axes[rotation].append((layer, lato))
    return RotationSpec(rot_to_axes=dict(rot_to_axes), axes_to_rot=axes_to_rot)


def mlp_rotation_spec_torch(num_hidden_layers: int) -> RotationSpec:
    assert num_hidden_layers >= 1

    spec = {}
    spec["fc1.weight"] = ("R_1", None)  # permutazione sulle righe (neuroni uscita), nessuna sulle colonne
    for i in range(2, num_hidden_layers + 1):
        spec[f"fc{i}.weight"] = (f"R_{i}", f"R_{i-1}")
    spec[f"fc{num_hidden_layers + 1}.weight"] = (None, f"R_{num_hidden_layers}")

    return rotation_spec_from_axes_to_rot(spec)


def get_rotated_param(rs: RotationSpec, rot, k: str, params, except_axis=None): #k sarebbe il nome del layer
    w = params[k]
    for axis, r in enumerate(rs.axes_to_rot[k]):  # axis = 0 o 1 , r è la stringa della rotazione da applicare
        if axis == except_axis:
            continue
        if r is not None:
            R=rot[r].float()
            if axis == 0:
                # ruota le righe (neuroni in uscita)
                w = R @ w
            else:
                # ruota le colonne (neuroni in ingresso)
                w = w @ R.T
    return w


def apply_rotations(rs: RotationSpec, rot, params):
    return {k: get_rotated_param(rs, rot, k, params) for k in params.keys()}


def weight_matching_2(rng,rs: RotationSpec,params_a: dict,params_b: dict,max_iter=100,init_rot=None,silent=False):

    rot_sizes = {r: params_a[axes[0][0]].shape[axes[0][1]] for r, axes in rs.rot_to_axes.items()}

    if init_rot is None:
        rot= {r: torch.eye(n) for r, n in rot_sizes.items()}
    else:
        rot = init_rot

    rot_names = list(rot.keys())

    for iteration in range(max_iter):
        progress = False

        for r in rot_names:
            n = rot[r].shape[0]
            A = torch.zeros((n, n), dtype=torch.float32)

            # Costruisci la matrice di similarità tra neuroni per rotazione r
            for wk, axis in rs.rot_to_axes[r]:
                w_a = params_a[wk]
                w_b = get_rotated_param(rs, rot, wk, params_b, except_axis=axis)

                w_a = w_a.transpose(0, axis).reshape(n, -1)
                w_b = w_b.transpose(0, axis).reshape(n, -1)

                A += w_a @ w_b.T

            U, _, Vt = torch.linalg.svd(A)
            R_new = U @ Vt
            oldL = torch.sum(rot[r] * A)
            newL = torch.sum(R_new * A)

            if newL > oldL + 1e-6:
                rot[r] = R_new
                progress = True

        if not progress:
            break

    return rot



def rotate_model(model_b,params_b,state_b, ps, perm):
    rotated_params_b = apply_rotations(ps, perm, params_b)
    rotated_state_b = state_b.copy()
    for k in rotated_params_b:
        rotated_state_b[k] = rotated_params_b[k]
    m_rotated = copy.deepcopy(model_b)
    m_rotated.load_state_dict(rotated_state_b)
    return m_rotated


def procrustes_on_layer(params_a, params_b, rot, layer, tol=1e-6):
    W_a = params_a[f'fc{layer}.weight']
    W_b = params_b[f'fc{layer}.weight']

    if layer == 1:
        Q_prev = torch.eye(W_a.shape[1], device=W_a.device, dtype=W_a.dtype)
    else:
        Q_prev = rot[f'R_{layer - 1}']

    M = W_a @ Q_prev @ W_b.T
    U, _, Vt = torch.linalg.svd(M)
    R_new = U @ Vt

    old_R = rot.get(f'R_{layer}', torch.eye(W_a.shape[0], device=W_a.device, dtype=W_a.dtype))
    old_sim = torch.sum(old_R * M)
    new_sim = torch.sum(R_new * M)

    if new_sim > old_sim + tol:
        return R_new
    else:
        return old_R


def weight_matching_procrustes_layer(rs, params_a, params_b, max_iter=100, init_rot=None, tol=1e-6):
    rot_sizes = {
        r: params_a[axes[0][0]].shape[axes[0][1]]
        for r, axes in rs.rot_to_axes.items()
    }

    if init_rot is None:
        rot = {r: torch.eye(n, device=params_a[next(iter(params_a))].device, dtype=params_a[next(iter(params_a))].dtype) for r, n in rot_sizes.items()}
    else:
        rot = init_rot

    for _ in range(max_iter):
        converged = True
        rng = random.Random(48)
        layers = list(range(1, 4))
        rng.shuffle(layers)
        for l in layers:
            R_old = rot[f'R_{l}']
            R_new = procrustes_on_layer(params_a, params_b, rot, l, tol=tol)
            if not torch.allclose(R_old, R_new, atol=tol):
                converged = False
            rot[f'R_{l}'] = R_new
        if converged:
            break

    return rot


def weight_cycle_consistency_error(model_a, model_b, R_dict, layer_names):
    errors = {}
    R_prev = None
    for i, layer in enumerate(layer_names):
        W_a = getattr(model_a, layer).weight.data
        W_b = getattr(model_b, layer).weight.data

        R_l = R_dict[layer]
        if i == 0:
            # primo layer: no R_prev
            W_b_reconstructed = R_l.T @ W_a
        else:
            W_b_reconstructed = R_l.T @ W_a @ R_prev

        error = torch.norm(W_b - W_b_reconstructed, p='fro')
        errors[layer] = error.item()
        R_prev = R_l
    return errors

"""# Cycle consistency

Analizziam due tipologie di cycle consistency: dato un modello B applichiamo ad esso una serie di permutazione ottenendo Bpermutato che dovrebbe essere simile a model A. Applicando le permutazioni inverse a B permutato riottengo B? E se le applico ad A ottengo un modello simile ad B?
La risposta a entrambe le domande è si: B e Bpermutato sono funzionalmente equivalenti (le varie permutazioni sono compensate correttamente), ovviamente le attivazioni saranno diverse (altrimenti che miglioramento ci sarebbe) ma si puo tornare indetro. Vale anche con le rotazioni?
"""

def rebuild(new_model,model_b,R_dict,layer_names=['fc1','fc2','fc3','fc4']):
    W_ricostruito=copy.deepcopy(new_model)
    R_prev=None
    for i, layer in enumerate(layer_names):
        W_a = getattr(new_model, layer).weight.data
        W_b = getattr(model_b, layer).weight.data
        b_a=getattr(new_model, layer).bias.data
        b_b=getattr(model_b, layer).bias.data
        if i == 0:
            R_l = R_dict[layer]
            W_b_reconstructed = R_l.T @ W_a
            b_b_reconstructed=R_l.T @ b_a
        elif i==3:
            W_b_reconstructed =  W_a @ R_prev
            b_b_reconstructed=b_a
        else:
            R_l = R_dict[layer]
            W_b_reconstructed = R_l.T @ W_a @ R_prev
            b_b_reconstructed=R_l.T @ b_a
        layer_perm = getattr(W_ricostruito, layer)
        with torch.no_grad():
            layer_perm.weight.data.copy_(W_b_reconstructed)
            layer_perm.bias.data.copy_(b_b_reconstructed)
        R_prev = R_l
    return W_ricostruito

#se new_model è il modello permutato vedo se, applicando le permutazioni inverse riottengo B
#b_permuted dovrebbe essere simile a w_a, quindi se applico queste permutazioni ad a dovrei ottenere  wb, o un modello, quanto diverso da quello


"""B e B permutato sono funzionalmente equivalenti. Vale lo stesso per B e B ruotato? Ovviamente no, a causa di non linearità, ma la loss è ancora valida?"""

def functional_error_log_softmax_kl(model_p, model_q, data_loader):   #pvera,qu approssimata
    model_p.eval()
    model_q.eval()
    total_kl = 0.0
    total_samples = 0

    with torch.no_grad():
        for inputs, _ in data_loader:
            log_probs_p = model_p(inputs)
            log_probs_q = model_q(inputs)

            probs_p = log_probs_p.exp()
            kl_batch = F.kl_div(log_probs_q, probs_p, reduction='batchmean')

            batch_size = inputs.size(0)
            total_kl += kl_batch.item() * batch_size
            total_samples += batch_size

    mean_kl = total_kl / total_samples
    return mean_kl

## Analisi introduttiva


def _debiased_dot_product_similarity_helper(
    xty, sum_squared_rows_x, sum_squared_rows_y, squared_norm_x, squared_norm_y,
    n):
  """Helper for computing debiased dot product similarity (i.e. linear HSIC)."""
  # This formula can be derived by manipulating the unbiased estimator from
  # Song et al. (2007).
  return (
      xty - n / (n - 2.) * sum_squared_rows_x.dot(sum_squared_rows_y)
      + squared_norm_x * squared_norm_y / ((n - 1) * (n - 2)))

def feature_space_linear_cka(features_x, features_y, debiased=False):
  """Compute CKA with a linear kernel, in feature space.
  This is typically faster than computing the Gram matrix when there are fewer
  features than examples.
  Args:
    features_x: A num_examples x num_features matrix of features.  (Z_a.T)
    features_y: A num_examples x num_features matrix of features.  (Z_b.T)
    debiased: Use unbiased estimator of dot product similarity. CKA may still be
      biased. Note that this estimator may be negative.
  Returns:
    The value of CKA between X and Y.
  """
  features_x = features_x - np.mean(features_x, 0, keepdims=True)
  features_y = features_y - np.mean(features_y, 0, keepdims=True)

  dot_product_similarity = np.linalg.norm(features_x.T.dot(features_y)) ** 2
  normalization_x = np.linalg.norm(features_x.T.dot(features_x))
  normalization_y = np.linalg.norm(features_y.T.dot(features_y))

  if debiased:
    n = features_x.shape[0]
    # Equivalent to np.sum(features_x ** 2, 1) but avoids an intermediate array.
    sum_squared_rows_x = np.einsum('ij,ij->i', features_x, features_x)
    sum_squared_rows_y = np.einsum('ij,ij->i', features_y, features_y)
    squared_norm_x = np.sum(sum_squared_rows_x)
    squared_norm_y = np.sum(sum_squared_rows_y)

    dot_product_similarity = _debiased_dot_product_similarity_helper(
        dot_product_similarity, sum_squared_rows_x, sum_squared_rows_y,
        squared_norm_x, squared_norm_y, n)
    normalization_x = np.sqrt(_debiased_dot_product_similarity_helper(
        normalization_x ** 2, sum_squared_rows_x, sum_squared_rows_x,
        squared_norm_x, squared_norm_x, n))
    normalization_y = np.sqrt(_debiased_dot_product_similarity_helper(
        normalization_y ** 2, sum_squared_rows_y, sum_squared_rows_y,
        squared_norm_y, squared_norm_y, n))

  return dot_product_similarity / (normalization_x * normalization_y)

def layer_similarity(model_a, model_b, valid_dl, layer_names):
    dizA = get_multiple_fc_activations(model_a, layer_names, valid_dl)
    dizB = get_multiple_fc_activations(model_b, layer_names, valid_dl)
    similarity = {}

    for layer in layer_names:
        Z_A = dizA[layer].cpu().numpy()
        Z_B = dizB[layer].cpu().numpy()
        similarity[layer] = feature_space_linear_cka(Z_A.T, Z_B.T)

    return similarity

from sklearn.decomposition import PCA

def visual_similarity(model_a, model_b, valid_dl, layer_names):
    dizA = get_multiple_fc_activations(model_a, layer_names, valid_dl)
    dizB = get_multiple_fc_activations(model_b, layer_names, valid_dl)
    for layer in layer_names:
        Z_A = dizA[layer].cpu().numpy()
        Z_B = dizB[layer].cpu().numpy()
        pca_A = PCA(n_components=2)
        pca_B = PCA(n_components=2)

        Z_A_pca = pca_A.fit_transform(Z_A)
        Z_B_pca = pca_B.fit_transform(Z_B)
        colors = Z_A_pca[:, 0]
        fig, axes = plt.subplots(1, 2, figsize=(8, 4))
        fig.suptitle(f"Layer: {layer} ", fontsize=14)

        axes[0].scatter(Z_A_pca[:, 0], Z_A_pca[:, 1], c=colors, cmap='viridis', s=10)
        axes[0].set_title("PCA attivazioni Modello A")
        axes[0].set_xlabel("PC1")
        axes[0].set_ylabel("PC2")

        axes[1].scatter(Z_B_pca[:, 0], Z_B_pca[:, 1], c=colors, cmap='viridis', s=10)
        axes[1].set_title("PCA attivazioni Modello B")
        axes[1].set_xlabel("PC1")
        axes[1].set_ylabel("PC2")

        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.show()

"""## Residual error"""

def residual_error(model_a, model_b, valid_dl, layer_names=['fc1', 'fc2', 'fc3'], epsilon=1e-8):
    dizA = get_multiple_fc_activations(model_a, layer_names, valid_dl)
    dizB = get_multiple_fc_activations(model_b, layer_names, valid_dl)

    error_pre, error_post, amplification = {}, {}, {}

    for layer in layer_names:
        Z_A = dizA[layer]
        Z_B = dizB[layer]

        err_pre = torch.mean((Z_A - Z_B) ** 2)
        error_pre[layer] = err_pre

        H_A = Z_A.clamp(min=0)
        H_B = Z_B.clamp(min=0)

        err_post = torch.mean((H_A - H_B) ** 2)
        error_post[layer] = err_post

        amplification[layer] = err_post / (err_pre + epsilon)

    return error_pre, error_post, amplification

